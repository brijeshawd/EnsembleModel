{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Content\n",
        "\n",
        "- **Overfitting-Underfit trade off**\n",
        "    - Stochastic Gradient Boosting\n",
        "    - Train time\n",
        "    - Impact of outliers\n",
        "    - Code walkthrough\n",
        "    - Feature importance\n",
        "\n",
        "- **XGBoost**\n",
        "    - Hyperparams\n",
        "    - Code walkthrough"
      ],
      "metadata": {
        "id": "DwsnWkFAB3Ts"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWx0suhpY-JD"
      },
      "source": [
        "# **Overfitting-Underfitting Trade-off**\n",
        "\n",
        "\n",
        "#### How can we Regularize the GBDT model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYeoxBouZNjz"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "Let's see what are the hyperparams in GBDT \n",
        "\n",
        "#### 1. $M$ - The number of base learners\n",
        "\n",
        "#### What will happen if M increases? \n",
        "As number of base learners increase, it is more likely that train error will go to 0. \n",
        "\n",
        "Hence, the model will overfit.\n",
        "\n",
        "**As $M$ increases, the model will overfit**\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVfhMWYXl1UB"
      },
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1OehWNYBxUvBszDv8-VGFKaS4aefZbVZH' >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ChofVKLRFsM"
      },
      "source": [
        "\n",
        "####2. The depth of base learners \n",
        "\n",
        "#### What will happen if depth increases?\n",
        "\n",
        "As depth increases,model will overfit\n",
        "\n",
        "**How?**\n",
        "\n",
        "Let's say we have deep DT for base learners i.e. overfit models.\n",
        "\n",
        "- In that case, Within 4 or 5 models, we'll crazily overfit to noise. \n",
        "\n",
        "Hence, we don't use deep DT as base learners\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHkavGD3QwM-"
      },
      "source": [
        "<img src='https://drive.google.com/uc?id=1xD61beXeLiAi_5i4dY7M6Gfx_LjHSwfP' >\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWyjr5YYjs9f"
      },
      "source": [
        "With great power comes great risk \n",
        "\n",
        "The biggest risk of GBDT is \n",
        "- it can overfit easily if we are not careful\n",
        "\n",
        "So, in order to avoid that, we introduce another level of regularization \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qP7MyxlUmOD6"
      },
      "source": [
        "<img src='https://drive.google.com/uc?id=1CSNOZVJYp3NyRSjMTxeMj06Quve82fD9' >\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrLgHI3rSp7b"
      },
      "source": [
        "#### 3. Shrinkage or Learning rate ($\\nu$)\n",
        "\n",
        "\n",
        "We add a regularization term $ùúà$\n",
        "to the model $F_m(x)$ s.t. \n",
        " * $F_m(x) = F_{m-1}(x) +\\sum_{i=1}^mùúà\\ Œ≥_mh_m(x)$ \n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "#### What will be the value of ŒΩ (nu)?\n",
        "This ùúà always lies between 0 and 1\n",
        "\n",
        "<br>\n",
        "\n",
        "#### What is ŒΩ doing intuitively? \n",
        "Intuitively, ŒΩ is shrinking the impact of $m^{th}$ model so that models don't overfit easily\n",
        "\n",
        "Think of it as an addition lever to control the overfit.\n",
        "\n",
        "<br>\n",
        "\n",
        "So, we can say that,\n",
        "\n",
        "**as ŒΩ increases, chances of model overfitting increases**\n",
        "\n",
        "\n",
        "\n",
        "Note: We have **same ùúà for all models in GBDT**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HUckhJlmXAZ"
      },
      "source": [
        "<img src='https://drive.google.com/uc?id=1zTQ_3nJ8nWZgUzCVTjJuAUoY_uQ-qY_N' >\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUj8eJfKmgDo"
      },
      "source": [
        "#### Question: What should be done to $M$ if ùúà is decreased to keep the performance same?\n",
        "\n",
        "Number of **base learners should be increased** because as we are decresing ùúà, \n",
        "- we are restricting base learners contribution \n",
        "- due to which the error increases \n",
        "\n",
        "\n",
        "We use ùúà even when the $Œ≥$ is there to avoid overfit, as there are high chances of overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kl17Us5ioDQy"
      },
      "source": [
        "## **Question: Can we add a regulariser to (gamma weights) Œ≥?**\n",
        "\n",
        "Yes we can add regularisation to vector $Œ≥$ whixh is \n",
        "$Œ≥$=[$Œ≥_1,Œ≥_2,Œ≥_3....Œ≥_M$]\n",
        "\n",
        "\n",
        "\n",
        "We can even add other terms controlling the number of leaf nodes which also becomes hyper parameter\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OxoWxektIwm"
      },
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1EtZi-SVT7_ARPEuI2cxJ1QNOAQl0GVYJ' >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha-3rVJ-tP3a"
      },
      "source": [
        "# **Stochastic Gradient Boosting** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A57y8oZgu0PF"
      },
      "source": [
        "#### What if we add randomization to GBDT ? \n",
        "\n",
        "This is what Stochastic Gradient boosting does.\n",
        "\n",
        "The stochhastic gradient boosting follow the same framework as GBDT and\n",
        "\n",
        "- **it also does row sampling and column sampling**\n",
        "\n",
        "This **randomisation acts as regularisation** here.\n",
        "\n",
        "Popular libraries such as Xgboost, LightGBM (discussed later) use this approach.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGQR2nNAvx4F"
      },
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1b0t8tmqt6ksDPEK2Uqe-Q01IXUFPmSfC' >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taXLTMrtv-jB"
      },
      "source": [
        "## **Train time**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaA33tW3yqWJ"
      },
      "source": [
        "GBDT is the sequential model, Hence it is slow to train\n",
        "\n",
        "#### How can we speed up GBDT?\n",
        "\n",
        "* Row sampling and column sampling can speed up the model a little\n",
        "* While building each base leraners we can apply **parallelisation**, i.e one sub-tree can be built on a core and the other sub-tree can be built on another core\n",
        "* Even for bulding a single tree, **we have to calculate entropy of many features, this can be parallelised**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_VqvCCl0mW9"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=12V2N2vzcO9mBSj40dWX_xqv_5QezuJYK' >"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ltSVMVdZTjcY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gOoX8XN2TjZb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wQvWfoMCTjUA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HEUnb3OdTjOo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBqvnZRr0k1s"
      },
      "source": [
        "# **Impact of outliers** \n",
        "\n",
        "#### Question: Will outliers impact GBDT?\n",
        "\n",
        "Outliers can impact GBDT\n",
        "\n",
        "- Each model is fit on the residual of the previous model\n",
        "- Outliers will have higher residual values\n",
        "- So, so gradient boosting will focus a large amount of its attention on reducing residual for these points.\n",
        "\n",
        "#### Question: How to handle outliers?\n",
        "1. Remove outliers\n",
        "2. Specialized loss functions to handle outliers\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow9M5e0tY3MC"
      },
      "source": [
        "Let us consider squared-loss which is $‚àë_{i=1}^n(y_i - yÃÇ_i)^2$\n",
        "\n",
        "Here, if there is an outlier the error increases quadratically because of squaring\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7kIv4VAcP4y"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1eWKV9YbLFQYGQCuSR_FGHANPDw68L84D' >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KViQydjca86"
      },
      "source": [
        "\n",
        "##### What changes can we make in the loss function so that the model be robust to outliers?\n",
        "\n",
        "* We can use **Root mean squared error**\n",
        "* We can use absolute values of the error \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuawTwrpeY0r"
      },
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1QrUznldwhmGMthsMG-mmuz7L89zXS9FC' >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzNXtSwgHYeX"
      },
      "source": [
        "#### Question: abs function is not differentiable at 0; how to handle that?\n",
        "   * As, already the error is 0, we simply use derivative of abs function @0 = 0 (computational hack)\n",
        "\n",
        "Huber loss can also be used.\n",
        " * This function be quadratic in some areas and linear in some\n",
        "   * As mentioned in the formuala, it uses the quadratic function in some range and liinear in the other range of values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krsWIaONee2V"
      },
      "source": [
        "<img src='https://drive.google.com/uc?id=1ryIZnNANcX_a2nQjeWvwibEaoQz1CMOn' >\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvplWXmGenbb"
      },
      "source": [
        "<img src='https://drive.google.com/uc?id=1-fA_MyJbXQbwq1yBygn0109CljpZD0VZ' >\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wat_HfJdHjCb"
      },
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1menZJ9cL_62wkbz-zD8W4uFbFJY5GgRk' >\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IFtMhjbkHW3"
      },
      "source": [
        "\n",
        "### Code walkthrough\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR5WW-zmkLOZ"
      },
      "source": [
        "Let's look at sklearn implementation for GBDT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b1KA2MqkKj3",
        "outputId": "2ce2a5f3-0895-4e3c-b18b-dd00d11f6e1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Iter       Train Loss   Remaining Time \n",
            "         1           1.0121            3.63m\n",
            "         2           0.7894            3.68m\n",
            "         3           0.6437            3.67m\n",
            "         4           0.5359            3.64m\n",
            "         5           0.4574            3.64m\n",
            "         6           0.3971            3.61m\n",
            "         7           0.3476            3.59m\n",
            "         8           0.3052            3.63m\n",
            "         9           0.2735            4.03m\n",
            "        10           0.2465            3.95m\n",
            "        20           0.1315            3.46m\n",
            "        30           0.0882            3.13m\n",
            "        40           0.0632            2.84m\n",
            "        50           0.0486            2.57m\n",
            "        60           0.0380            2.33m\n",
            "        70           0.0299            2.06m\n",
            "        80           0.0238            1.80m\n",
            "        90           0.0194            1.52m\n",
            "       100           0.0156            1.25m\n",
            "Time taken for training : 0:03:36.244311\n",
            "Training accuracy:1.0\n",
            "Test Accuracy: 0.9553639360892722\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
        "\n",
        "start = dt.datetime.now()\n",
        "model3 = GBC(n_estimators=150, learning_rate=0.2, max_depth=4, random_state=0, verbose = 1).fit(X_train, Y_train)\n",
        "end = dt.datetime.now()\n",
        "\n",
        "print(f\"Time taken for training : {end - start}\\nTraining accuracy:{model3.score(X_train, Y_train)}\\nTest Accuracy: {model3.score(X_test, Y_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DS29SdpmmmEE"
      },
      "source": [
        "#### Plotting loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxUJF25HmRx_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "ab1ed505-ea79-4ffd-ced8-7f638e99b480"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.plot(model3.train_score_)\n",
        "\n",
        "plt.xlabel('Iter')\n",
        "plt.ylabel('loss')\n",
        "plt.title('Iter vs loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhx6tu0Fs1vD"
      },
      "source": [
        "# **Feature Importance**\n",
        "\n",
        "#### How do we find feature importance in GBDT ? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkgJx7ePwgHS"
      },
      "source": [
        "We simply find the feature importance of feature in each tree and \n",
        "- then take the average of these values to get over-all feature importance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoHHWmNvxGXL"
      },
      "source": [
        "\n",
        "<img src='https://drive.google.com/uc?id=1NW-6jIIU55duZGJGzEIUk5P-8QZ3DZ-m' width = 600 >\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHbO_74uyf3p"
      },
      "source": [
        "#### Code walkthrough"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuQ_R554x4HC"
      },
      "outputs": [],
      "source": [
        "print(model3.feature_importances_)\n",
        "\n",
        "plt.bar(range(len(model3.feature_importances_)), model3.feature_importances_)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnrIE_4-kfIM"
      },
      "source": [
        "Notice,\n",
        "- It took around 4 min to train a single model\n",
        "- Imagine how much time it'll take if we do hyperparam tuning on it.\n",
        "\n",
        "Sklearn's implemenation of GBDT is not optimized and hence not preferred.\n",
        "\n",
        "**So, what library do we use then?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLmzSefbbir2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VMxtJrTbipW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqJbPs7obinA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Pe4czVs1kVS"
      },
      "source": [
        "# **Xgboost**\n",
        "\n",
        "XGBoost is one the popular library known for its well optimized code\n",
        "\n",
        "For example : \n",
        "\n",
        "- if you have numerical feature $f_i$, \n",
        "    - instead of tryinig all the values for thresholding, \n",
        "    - it build a histogram of data and use simple rules like quartiles and percentiles to make thresholding.\n",
        "\n",
        "- It also does multi core optimization (parallelization)\n",
        "    - it'll compute each branch of a base learner on different core to speed up the process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGdVVzudUx0K"
      },
      "source": [
        "### **Xgboost hyperparams**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHwaJ4RCU2HS"
      },
      "source": [
        "1. **Eta**: or the learning rate is the shrinking/regularization term which we studier earlier \n",
        "\n",
        "2. **min_split_loss** specify the minimum Information Gain which you want for further split. \n",
        "\n",
        "#### Question: What happens if min_split_loss increases?\n",
        "If the min_split_loss value of the model is increased, \n",
        "- the splitting stops if the min_split_loss is not met. \n",
        "    \n",
        "Due to this the depth decreases resulting in shallow tress.\n",
        "\n",
        "Hence,results in the underfitting of the model.\n",
        "\n",
        "\n",
        "3. **max_depth**, this parameter is used to set the depth of the base learners  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OShdCyR2XCbt"
      },
      "source": [
        "4. **min_child_weight**: you can increase the weight of the child due to which the splitting stops if the required threshold is not met"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1X8kzQ0UX8-7"
      },
      "source": [
        "5. **subsample**. it's nothing but the row sampling \n",
        " * There are many ways of sampling, same with column sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0n-UauKYoym"
      },
      "source": [
        "6. **Lambda**, it is the used apply the L2 regularization on the weights, which is $Œ≥$ in our case \n",
        "\n",
        "7. **Tree_methods**, there are many ways to build a tree, we can specify the method basing on the conditions "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckHd2kWIXFH0"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1tvyj1KYedvVhaeIJIAh_ZillGmf5p5rE' >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjHHBAPwXizX"
      },
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1k_JD_dg0las0-N03UWQ_jSzkIP1M-b9W' >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUoUNjITZFFv"
      },
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1Vvz_rxFrx0EAgwClXn8OFwkhZ_5eD9C0' >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2S8tbitZzTU"
      },
      "source": [
        "#### What are most used hyperparams?\n",
        "\n",
        "- Number of estimators (M)\n",
        "- Depth\n",
        "- ŒΩ : learning rate\n",
        "- Col sampling/ row sampling \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qGNJ2_pacN7"
      },
      "source": [
        "\n",
        "\n",
        "<img src='https://drive.google.com/uc?id=1n0JuWHd8NerqKnYnWFQqIwKeNuJG7PUr' >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvsDq-PSfS29"
      },
      "source": [
        "### Code walkthrough"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HiwuBmp0fUqI"
      },
      "outputs": [],
      "source": [
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import datetime as dt\n",
        "\n",
        "params = {\n",
        "        'learning_rate': [0.1, 0.5, 0.8],\n",
        "        'subsample': [0.6, 0.8, 1.0],\n",
        "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "        'max_depth': [3, 4, 5]\n",
        "        }\n",
        "xgb = XGBClassifier(n_estimators=100, objective='multi:softmax', num_class=20, silent=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isosAJtbfZr7"
      },
      "outputs": [],
      "source": [
        "folds = 3\n",
        "\n",
        "skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
        "\n",
        "random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=10, scoring='accuracy', n_jobs=4, cv=skf.split(X_train,Y_train), verbose=3, random_state=1001 )\n",
        "\n",
        "\n",
        "start = dt.datetime.now()\n",
        "random_search.fit(X_train, Y_train)\n",
        "end = dt.datetime.now()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxwNTFHdfiJQ"
      },
      "outputs": [],
      "source": [
        "print('\\n Best hyperparameters:')\n",
        "print(random_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UF4equbhfnZE"
      },
      "outputs": [],
      "source": [
        "best_xgb = XGBClassifier(n_estimators=100, objective='multi:softmax', num_class=20, subsample=0.8, max_depth=5, learning_rate=0.5, colsample_bytree=1.0, silent=True)\n",
        "best_xgb.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyAGwdadfsr-"
      },
      "outputs": [],
      "source": [
        "print(f\"Time taken for training : {end - start}\\nTraining accuracy:{best_xgb.score(X_train, Y_train)}\\nTest Accuracy: {best_xgb.score(X_test, Y_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFPM3x6NlX-7"
      },
      "source": [
        "Since we are using randomized Search\n",
        "- we made total of 30 fits \n",
        "    - total 10 combination of hyperparam\n",
        "    - 3 fold cv for each combination\n",
        "\n",
        "All these 30 fits took mere 5 mins to run compared to 4 mins for single fit of sklearn GBDT.\n",
        "\n",
        "**Do you now see how fast it is compared to sklearn implementation?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyAhXx1cytvk"
      },
      "source": [
        "#### Feature importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2v-sFCtyvXe"
      },
      "outputs": [],
      "source": [
        "print(best_xgb.feature_importances_)\n",
        "\n",
        "plt.bar(range(len(best_xgb.feature_importances_)), best_xgb.feature_importances_)\n",
        "plt.show()"
      ]
    }
  ]
}